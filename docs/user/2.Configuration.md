### Table of Contents
1. [Configuration]()
   - [AI Assistant](#ai-assistant)
   - [API](#api)
   - [RAG](#rag)

---

# Configuration

The setup of AI-OPS Agent and API can be customized with environment variables loaded from a `.env` file; 
there are three classes of configuration settings: **Assistant**, **API** and **RAG** (*in-development*).

To override the default configuration values, create a `.env` file in the root of your project directory. 
This file can include any of the settings listed above. The `.env` file should look something like this:

```
# example .env
MODEL=mistral:7b-instruct-v0.3-q8_0
ENDPOINT=http://your-ollama-endpoint
```


### AI Assistant

| Variable     | Description                                                                                                                                                                                                                                                                                                   |
|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **MODEL**    | The model used by the AI assistant. This should be set to the exact name of the model as recognized by Ollama. For example, if you are using mistral:7b-instruct-v0.3-q8_0, set MODEL=mistral:7b-instruct-v0.3-q8_0. <br> *Default value: `mistral`*                                                          |                                                                                                                                                                                                                      |
| **ENDPOINT** | The endpoint where the Ollama service is running. This is used for both the AI agent model and the embedding model (for RAG). In most local deployments, this will point to `http://localhost:11434`, but you should change it if Ollama is deployed elsewhere. <br>*Default value: `http://localhost:11434`* |

Additional variables will be used in the future such as:
- **USE_RAG**: If set to True, this enables the Retrieval-Augmented Generation (RAG) functionality, which allows the agent to fetch information from a vector database.
- **PROVIDER** and **PROVIDER_KEY**: The model provider and an eventual required api key. *Currently only ollama is supported*, but this parameter is in place for future support of additional providers.


### API

| Variable    | Description                                                                                                                                                            |
|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **PROFILE** | If set to True, enables profiling of system performance, such as memory usage. This setting can be useful for debugging and development. <br> *Default value: `False`* |

### RAG

**Note**: RAG functionality is in active development and may not be fully stable. It is recommended that users prefer persistent Qdrant instances when using RAG.

| **Setting**       | **Description**                                                                                                                                                              |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `RAG_URL`         | The URL of the Qdrant vector database (local or remote). <br> *Default Value:* `http://localhost:6333`                                                                       |
| `IN_MEMORY`       | If set to `True`, the vector database will be kept in memory, which is suitable for lightweight testing but not recommended for large datasets. <br> *Default Value:* `True` |
| `EMBEDDING_MODEL` | The model used for generating embeddings, should be an Ollama-compatible model. <br> *Default Value:* `nomic-embed-text`                                                     |
| `EMBEDDING_URL`   | The URL of the embedding service, typically the same as the `ENDPOINT` URL. <br> *Default Value:*  `http://localhost:11434`                                                  |

