{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.core.llm import Ollama\n",
    "from test.evaluation.utils import JudgeLLM\n",
    "\n",
    "load_dotenv()\n",
    "MODEL = os.getenv('MODEL')\n",
    "ENDPOINT = os.getenv('ENDPOINT')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "MODEL = Ollama(model=MODEL, inference_endpoint=ENDPOINT)\n",
    "\n",
    "# pre-load LLM\n",
    "for ch in MODEL.query([{'role': 'user', 'content': 'Hi'}]):\n",
    "    pass\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "judge_llm = JudgeLLM()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.317369Z",
     "start_time": "2024-11-28T17:55:27.045661Z"
    }
   },
   "id": "6953b50a3689e61a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are an expert Penetration Tester and cybersecurity expert, here to help the user during a penetration testing assessment.\n",
    "Your response should only contain a tool call, without additional information or explanation. You have access ONLY to the following tools:\n",
    "{'type': 'function', 'function': {'name': 'search_web', 'parameters': {'type': 'object', 'properties': {'search_query': {'type': 'string'}}, 'required': ['search_query']}, 'description': 'Make a search with Google.'}}\n",
    "\n",
    "You should provide a tool call following EXACTLY the following format:\n",
    "{'name': 'function_name', 'parameters': {'param1': 'content of param1', ...}}\n",
    "\n",
    "RULES:\n",
    "- Your response contains ONLY one tool call in the provided format.\n",
    "- Your response can optionally be \"{}\" if no tool call is required.\n",
    "- STOP after writing the tool call, without additional information.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.323869Z",
     "start_time": "2024-11-28T17:55:32.318870Z"
    }
   },
   "id": "6033d695c358c1ff",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PROMPT.replace(\"'\", '\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.329868Z",
     "start_time": "2024-11-28T17:55:32.324870Z"
    }
   },
   "id": "a24ba851e72d8ca5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TESTS = [\n",
    "    {\"input\": \"Hi\"}, \n",
    "    {\"input\":  \"While experimenting with SQLi I found the following ways to bypass a client-side filter:\\nOR: OORR\\nAND: AANDND\\nSELECT: SESELECTLECT\\nUNION: UNUNIONION\\nORDER BY: OORRDER BY\\n\\nWrite the tamper script for sqlmap to use this filter bypass techniques.\"},\n",
    "    {\"input\": \"Are there any tools to automate Server Side Template Injection?\"}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.336374Z",
     "start_time": "2024-11-28T17:55:32.331368Z"
    }
   },
   "id": "fb1140dfd568c595",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.346374Z",
     "start_time": "2024-11-28T17:55:32.337367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correct Format\n",
    "# Same code that is in architecture implementation\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from src.core.tools import TOOL_REGISTRY\n",
    "from src.utils import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "tools = TOOL_REGISTRY.marshal('base')\n",
    "tool_pattern = r\"\\s*({[^}]*(?:{[^}]*})*[^}]*}|\\[[^\\]]*(?:\\[[^\\]]*\\])*[^\\]]*\\])\\s*$\"\n",
    "\n",
    "\n",
    "def extract_tool_call(\n",
    "    tool_call_response: str\n",
    ") -> Tuple[str | None, Dict]:\n",
    "    \"\"\"Extracts the tool call and its parameters from the LLM response.\n",
    "\n",
    "    :param tool_call_response: The response containing a tool call.\n",
    "\n",
    "    :returns:\n",
    "        (tool name, parameters) OK\n",
    "        (None, None) if extraction fails.\"\"\"\n",
    "    tool_call_match = re.search(tool_pattern, tool_call_response)\n",
    "    if not tool_call_match:\n",
    "        logger.error(\n",
    "            f'Tool call failed: not found in LLM response: {tool_call_response}'\n",
    "        )\n",
    "        return None, {}\n",
    "    try:\n",
    "        # fix response to be JSON\n",
    "        tool_call_json = tool_call_match \\\n",
    "            .group(1) \\\n",
    "            .replace('\"', '')\n",
    "        tool_call_json = tool_call_json \\\n",
    "            .replace(\"'\", '\"')\n",
    "\n",
    "        tool_call_dict = json.loads(tool_call_json)\n",
    "        name, parameters = tool_call_dict['name'], tool_call_dict['parameters']\n",
    "    except json.JSONDecodeError as json_extract_err:\n",
    "        logger.error(\n",
    "            f'Tool call failed: not found in LLM response: {tool_call_response}'\n",
    "            f'\\nError: {json_extract_err}'\n",
    "        )\n",
    "        return None, {}\n",
    "\n",
    "    # check if tool exists\n",
    "    found = False\n",
    "    for t in tools:\n",
    "        if t['function']['name'] == name:\n",
    "            found = True\n",
    "    if not found:\n",
    "        logger.error(f'Tool call failed: {name} is not a tool.')\n",
    "        return None, {}\n",
    "    return name, parameters"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Relevance\n",
    "\n",
    "import deepeval\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    BaseMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    PromptAlignmentMetric\n",
    ")\n",
    "\n",
    "deepeval.telemetry_opt_out()\n",
    "\n",
    "metrics: dict[str: BaseMetric] = {\n",
    "    'relevance': AnswerRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=JudgeLLM(),\n",
    "        include_reason=True\n",
    "    ),\n",
    "    'prompt_alignment': PromptAlignmentMetric(\n",
    "        threshold=0.7,\n",
    "        model=JudgeLLM(),\n",
    "        include_reason=True,\n",
    "        prompt_instructions=[PROMPT]\n",
    "    )\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:55:32.352369Z",
     "start_time": "2024-11-28T17:55:32.347869Z"
    }
   },
   "id": "7b7ccaa00682afad",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f80245da1f3b4c709e4a47b685c65800"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:191\u001B[0m, in \u001B[0;36mAnswerRelevancyMetric._a_generate_verdicts\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 191\u001B[0m     res: Verdicts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ma_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVerdicts\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [item \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39mverdicts]\n",
      "\u001B[1;31mTypeError\u001B[0m: JudgeLLM.a_generate() got an unexpected keyword argument 'schema'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\utils.py:244\u001B[0m, in \u001B[0;36mtrimAndLoadJson\u001B[1;34m(input_string, metric)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjsonStr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m json\u001B[38;5;241m.\u001B[39mJSONDecodeError:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[1;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:353\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[1;34m(self, s, idx)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[1;31mJSONDecodeError\u001B[0m: Invalid \\escape: line 5 column 39 (char 85)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 26\u001B[0m\n\u001B[0;32m     20\u001B[0m deep_eval_test_case \u001B[38;5;241m=\u001B[39m LLMTestCase(\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtest[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     22\u001B[0m     actual_output\u001B[38;5;241m=\u001B[39mresponse\n\u001B[0;32m     23\u001B[0m )\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric_name, metric \u001B[38;5;129;01min\u001B[39;00m metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 26\u001B[0m     \u001B[43mmetric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmeasure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeep_eval_test_case\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     TESTS[i][metric_name] \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39mscore\n\u001B[0;32m     28\u001B[0m     TESTS[i][\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_reason\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39mreason\n",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:58\u001B[0m, in \u001B[0;36mAnswerRelevancyMetric.measure\u001B[1;34m(self, test_case, _show_indicator)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masync_mode:\n\u001B[0;32m     57\u001B[0m     loop \u001B[38;5;241m=\u001B[39m get_or_create_event_loop()\n\u001B[1;32m---> 58\u001B[0m     \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ma_measure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_case\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_show_indicator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatements: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_statements(\n\u001B[0;32m     63\u001B[0m         test_case\u001B[38;5;241m.\u001B[39mactual_output\n\u001B[0;32m     64\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[1;34m(self, future)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     97\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent loop stopped before Future completed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__log_traceback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception_tb)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:267\u001B[0m, in \u001B[0;36mTask.__step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;66;03m# We use the `send` method directly, because coroutines\u001B[39;00m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mcoro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    269\u001B[0m         result \u001B[38;5;241m=\u001B[39m coro\u001B[38;5;241m.\u001B[39mthrow(exc)\n",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:99\u001B[0m, in \u001B[0;36mAnswerRelevancyMetric.a_measure\u001B[1;34m(self, test_case, _show_indicator)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m metric_progress_indicator(\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m, async_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, _show_indicator\u001B[38;5;241m=\u001B[39m_show_indicator\n\u001B[0;32m     94\u001B[0m ):\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatements: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_a_generate_statements(\n\u001B[0;32m     96\u001B[0m         test_case\u001B[38;5;241m.\u001B[39mactual_output\n\u001B[0;32m     97\u001B[0m     )\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverdicts: List[AnswerRelvancyVerdict] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m---> 99\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_a_generate_verdicts(test_case\u001B[38;5;241m.\u001B[39minput)\n\u001B[0;32m    100\u001B[0m     )\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscore \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calculate_score()\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_a_generate_reason(test_case\u001B[38;5;241m.\u001B[39minput)\n",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:197\u001B[0m, in \u001B[0;36mAnswerRelevancyMetric._a_generate_verdicts\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    196\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39ma_generate(prompt)\n\u001B[1;32m--> 197\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mtrimAndLoadJson\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    199\u001B[0m         AnswerRelvancyVerdict(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mitem) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mverdicts\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    200\u001B[0m     ]\n",
      "File \u001B[1;32mD:\\Desktop\\prog\\Projects\\AI-OPS\\.venv\\Lib\\site-packages\\deepeval\\metrics\\utils.py:249\u001B[0m, in \u001B[0;36mtrimAndLoadJson\u001B[1;34m(input_string, metric)\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metric \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    248\u001B[0m         metric\u001B[38;5;241m.\u001B[39merror \u001B[38;5;241m=\u001B[39m error_str\n\u001B[1;32m--> 249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_str)\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn unexpected error occurred: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model."
     ]
    }
   ],
   "source": [
    "for i, test in enumerate(TESTS):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': PROMPT},\n",
    "        {'role': 'user', 'content': test['input']}\n",
    "    ]\n",
    "    \n",
    "    response = ''\n",
    "    for chunk, _ in MODEL.query(messages):\n",
    "        response += chunk\n",
    "    TESTS[i]['response'] = response\n",
    "        \n",
    "    # correctness\n",
    "    tool_name, tool_parameters = extract_tool_call(response)\n",
    "    if tool_name is None:\n",
    "        TESTS[i]['format'] = 0 # format not correct\n",
    "    else:\n",
    "        TESTS[i]['format'] = 1 # format correct\n",
    "    \n",
    "    # metrics\n",
    "    deep_eval_test_case = LLMTestCase(\n",
    "        input=test['input'],\n",
    "        actual_output=response\n",
    "    )\n",
    "    \n",
    "    for metric_name, metric in metrics.items():\n",
    "        metric.measure(deep_eval_test_case)\n",
    "        TESTS[i][metric_name] = metric.score\n",
    "        TESTS[i][f'{metric_name}_reason'] = metric.reason"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T17:57:43.570807Z",
     "start_time": "2024-11-28T17:57:38.627406Z"
    }
   },
   "id": "dee4a97441b6eab4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(TESTS)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0a803113b180c83",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7c276569005672bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
